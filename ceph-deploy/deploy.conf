[global]
filestore_fd_cache_size = 204800
filestore_omap_header_cache_size = 204800

osd pool default pg num = 512
osd pool default pgp num = 512
osd pool default size = 3
osd pool default min size = 2

### MUST Configure network value used by ceph
#public network = 10.209.242.0/24
#cluster network = 10.209.243.0/24

## allow ourselves to open a lot of files
max open files = 320000

## osds check heartbeats
# osd_heartbeat_interval : interval to check heartbeat of other OSD Daemon
#          Default value : 6 secs
# osd_heartbeat_grace    : osd grace period to mark OSD Down
#          Default value : 20 secs
# So, osd will report another OSD Down every 'osd_heartbeat_grace' secs
osd_heartbeat_interval = 10
osd_heartbeat_grace = 60

## osds report down osds
# mon_osd_min_down_reports   : minmum number of 'OSD Down' reports
#              Default value : 3 times
# mon_osd_min_down_reporters : minmum number of osds required to report a OSD Down
#              Default value : 1
mon_osd_min_down_reports = 5
mon_osd_min_down_reporters = 3

## osds report there status
# mon_osd_report_timeout    : Secs before Monitor consider OSD Down
# mon_osd_down_out_interval : Secs before Monitor consider OSD Down and Out
mon_osd_report_timeout = 800
mon_osd_down_out_interval = 1000

log_to_syslog = false        ; uncomment this line to log to syslog
ms_nocrc = true
#ms_bind_port_min = 800
#ms_bind_port_max = 1000

filestore_fiemap = true

rgw_frontends = "civetweb port=10080 num_threads=2000"
rgw_override_bucket_index_max_shards = 20
rgw_thread_pool_size = 512
rgw_max_chunk_size = 1048576
rgw_cache_lru_size = 1000000

##################
## Monitors
## You need at least one. You need at least three if you want to
## tolerate any node failures. Always create an odd number.
[mon]
mon clock drift allowed    = .15
mon_pg_warn_max_object_skew = 0
mon osd full ratio = 0.90
mon osd nearfull ratio = 0.80

[osd]
# osd journal = /osd-journal-path/osd$id/journal
osd journal size = 20480    ;journal size, in megabytes

osd_recovery_threads = 1
osd recovery max active = 2
osd_max_backfills = 2
osd_recovery_max_chunk = 4194304
osd_recovery_op_priority = 5

osd mkfs type = xfs
osd mount options xfs = rw,noexec,noatime,attr2,inode64,logbufs=8,logbsize=256k,noquota
#osd mkfs options xfs = -f -i 2048

osd op threads = 4
osd disk threads = 2
osd_map_cache_size = 1024
osd_op_num_threads_per_shard = 2
osd_op_num_shards = 15

osd_op_thread_timeout = 5
osd op history size = 20

filestore queue max ops = 50000
filestore queue max bytes = 10485760000
filestore queue committing max bytes = 10485760000
filestore queue committing max ops = 50000
filestore op threads = 8
filestore journal writeahead = true

## filesytem config
filestore_wbthrottle_xfs_ios_start_flusher = 10000000
filestore_wbthrottle_xfs_ios_hard_limit = 10000000
filestore_wbthrottle_xfs_inodes_start_flusher = 10000
filestore_wbthrottle_xfs_inodes_hard_limit = 10000
filestore_wbthrottle_xfs_bytes_start_flusher = 419430400
filestore_wbthrottle_xfs_bytes_hard_limit = 4194304000

journal max write entries = 50000
journal queue max ops = 50000
objecter inflight ops = 819200
journal max write bytes = 10485760000
journal queue max bytes = 10485760000
ms dispatch throttle bytes = 1048576000
objecter inflight op bytes = 1048576000
osd_client_message_cap = 1000000
osd_client_message_size_cap = 2147483648
osd_max_write_size = 512

## manage the CRUSH map entirely manually
## only need configure this after we had set our crushmap
#osd crush update on start = false


[client]
rbd cache = true
rbd cache size = 67108864
rbd cache max dirty = 33554432
rbd cache max dirty age = 5
rbd cache writethrough until flush = true
rbd_default_order = 25
rbd_default_stripe_unit = 4194304
rbd_default_stripe_count = 8

##################
## host-specific
## Put your settings for each node here, these settings will be removed
## from the ultimate ceph.conf
[host-specific]
mon_hosts = CDM3E07-209242021, CDM3E07-209242022, CDM3E08-209242035
osd_hosts = CDM3E07-209242021, CDM3E07-209242022, CDM3E08-209242035
rgw_hosts = CDM3E07-209242021, CDM3E07-209242022, CDM3E08-209242035

# Example configuration use file system path
#osd.CDM3E08-209242035.paths = /var/lib/ceph/osd/ceph-0, /var/lib/ceph/osd/ceph-1
#osd.CDM3E08-209242035.paths = /var/lib/ceph/osd/ceph-2, /var/lib/ceph/osd/ceph-3
osd.CDM3E08-209242035.devs = sdb:sdl, sdc:sdl, sdd:sdl, sde:sdl, sdf:sdl, sdg:sdm, sdh:sdm, sdi:sdm, sdj:sdm, sdk:sdm
osd.CDM3E07-209242022.devs = sdb:sdl, sdc:sdl, sdd:sdl, sde:sdl, sdf:sdl, sdg:sdm, sdh:sdm, sdi:sdm, sdj:sdm, sdk:sdm
osd.CDM3E07-209242021.devs = sdb:sdl, sdc:sdl, sdd:sdl, sde:sdl, sdf:sdl, sdg:sdm, sdh:sdm, sdi:sdm, sdj:sdm, sdk:sdm

## for multi network settings
## public network 1;cluster network1 = Host1, Host2, Host3
## public network 2;cluster network2 = Host4, Host5
10.209.242.0/24;10.209.243.0/24 = CDM3E07-209242021, CDM3E07-209242022
10.209.244.0/24;10.209.245.0/24 = CDM3E08-209242035
